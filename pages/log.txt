{"id":{"value":"form1_table3_tr1_td1_table5_tr1_td1_table1_tr1_td2"},"start":{"value":22977},"end":{"value":24779},"azione":{"value":"I"},"object":{"value":"http://salt.semanticauthoring.org/ontologies/sro#Conclusion"},"email":{"value":"i%40i.i"},"at":{"value":"8/2/2016"},"label":{"value":"Conclusione"},"subject":{"value":"ver1"},"predicate":{"value":"http://www.ontologydesignpatterns.org/cp/owl/semiotics.owl#denotes"},"bLabel":{"value":"Conclusione"},"key":{"value":"In this paper we have presented a method for a large scale citation blocking using hash functions. A blocking workflow implementable in map-reduce paradigm has been demonstrated. Various hash functions were presented and evaluated.\n\nEach of them has its strong and weak sides, which make them fit different scenarios. When citation strings contain document titles, it is probably the best idea to use bigram-based hashes. name-year-pages and name-year-numn will be preferable when dealing with references to journal publications which contain various numbers (e.g. pages, issue, volume) enabling straightforward document identification. name-year method is probably the most generic one, offering very good trade-off between recall and the amount of resources used.\n\nThanks to diversity of hash function types, they can be combined to form a new, better method, although one must carefully examine if it really helps. Nevertheless, with a bit of caution one can greatly improve a resource-efficiency without much loss in terms of recall.\n\nAs it has been mentioned, in the future it would be very interesting to experiment with alternative methods of combining hash functions. Probably the better results can be achieved when subsequent round of hash generation is executed after exact matching step, when it is more certain if a citation needs further matching. On the other hand, maybe some various hashing methods may be used simultaneously in one heuristic round. Small yet smart optimisations in hash generation process may also improve the effectiveness of the method. Deeper analysis and simple parsing of citation string may be incorporated. Finally, the scalability of our solution should be investigated even further: what today is considered a big scale is just an ordinary scale of tomorrow."},"name":{"value":"Ion Ursachi"}}|{"id":{"value":"form1_table3_tr1_td1_table5_tr1_td1_table1_tr1_td2"},"start":{"value":1069},"end":{"value":3108},"azione":{"value":"I"},"object":{"value":"http://purl.org/spar/deo/Introduction"},"email":{"value":"i%40i.i"},"at":{"value":"8/2/2016"},"label":{"value":"Introduzione"},"subject":{"value":"ver1"},"predicate":{"value":"http://www.ontologydesignpatterns.org/cp/owl/semiotics.owl#denotes"},"bLabel":{"value":"Introduzione"},"key":{"value":"Record deduplication, entity resolution, data linking, object matching are all names of similar processes. As the amounts of data we are dealing with grow larger, the ability to tell which objects are unique and which are merely duplicates is becoming increasingly important. It is also interesting that the challenges we face change as the scale of our work changes. Different problems will arise depending on whether we are to deduplicate one thousand or one million records.\n\nNevertheless, the basic data linking workflow is usually unaltered. First of all, the data is grouped in so called blocks. We assume no entries in two distinct blocks will be deduplicated, i.e. merged. Then, similarity among all pairs in a given blocks is computed and objects are clustered using an arbitrary algorithm. Although other indexing techniques can be used instead of blocking, as demonstrated by Christen [1], the former seems to remain most commonly used. There are many tools implementing this workflow [8], also in distributed environment [9, 11]). \n\nIn this paper, we will focus on citation matching task, i.e. creating links between a bibliography entry and the referenced publication record in a database. Assuming that a citation is, in a sense, a duplicate of referenced document metadata record the problem is reduced to the deduplication task. Citation matching has been studied for a long time [3, 7], also using big data tools [4] yet some issues remain unsolved. One of them is blocking crafted for this particular task. That will be the main concern of this article.\n\nA blocking method using hash functions shall be outlined. We will propose a basic blocking workflow to be implemented using map-reduce paradigm. Numerous blocking key generation techniques shall be described, compared and evaluated. The feasibility of their usage with big data will be emphasised. Combinations of various blocking techniques will be demonstrated. Finally, some technicalities will be revealed and a full citation matching workflow will be presented."},"name":{"value":"Ion Ursachi"}}